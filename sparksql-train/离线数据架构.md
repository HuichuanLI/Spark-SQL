## 用户行为日志
    用户每次访问网站时所有的行为数据（访问、浏览、搜索、点击...）
   	
   
### 日志数据内容：
   - 1）访问的系统属性： 操作系统、浏览器等等
   - 2）访问特征：点击的url、从哪个url跳转过来的(referer)、页面上的停留时间等
   - 3）访问信息：session_id、访问ip(访问城市)等
   
### 数据处理流程

    数据处理流程
    1）数据采集
    	Flume： web日志写入到HDFS
    	Nginx 代码
    
    2）数据清洗
    	脏数据
    	Spark、Hive、MapReduce 或者是其他的一些分布式计算框架  
    	清洗完之后的数据可以存放在HDFS(Hive/Spark SQL)
    
    3）数据处理
    	按照我们的需要进行相应业务的统计和分析
    	Spark、Hive、MapReduce 或者是其他的一些分布式计算框架
    
    4）处理结果入库
    	结果可以存放到RDBMS、NoSQL
    
    5）数据的可视化
    	通过图形化展示的方式展现出来：饼图、柱状图、地图、折线图
    	ECharts、HUE、Zeppelin

    一般的日志处理方式，我们是需要进行分区的，
    按照日志中的访问时间进行相应的分区，比如：d,h,m5(每5分钟一个分区)
    
    
    输入：访问时间、访问URL、耗费的流量、访问IP地址信息
    输出：URL、cmsType(video/article)、cmsId(编号)、流量、ip、城市信息、访问时间、天

    使用github上已有的开源项目
    1）git clone https://github.com/wzhe06/ipdatabase.git
    2）编译下载的项目：mvn clean package -DskipTests
    3）安装jar包到自己的maven仓库
    mvn install:install-file -Dfile=/Users/rocky/source/ipdatabase/target/ipdatabase-1.0-SNAPSHOT.jar -DgroupId=com.ggstar -DartifactId=ipdatabase -Dversion=1.0 -Dpackaging=jar

## 数据调优
    调优点：
    1) 控制文件输出的大小： coalesce
    2) 分区字段的数据类型调整：spark.sql.sources.partitionColumnTypeInference.enabled
    3) 批量插入数据库数据，提交使用batch操作
    
    
    语法：ROW_NUMBER() OVER(PARTITION BY COLUMN ORDER BY COLUMN)
    简单的说row_number()从1开始，为每一条分组记录返回一个数字，这里的ROW_NUMBER() OVER (ORDER BY xlh DESC) 是先把xlh列降序，再为降序以后的没条xlh记录返回一个序号。
    
    
    
## 数据可视化
    一副图片最伟大的价值莫过于它能够使得我们实际看到的比我们期望看到的内容更加丰富
 
     常见的可视化框架
     1）echarts
     2）highcharts
     3）D3.js
     4）HUE 
     5）Zeppelin
     
## 数据优化

   - 存储格式
   传统的关系型数据库，如 Oracle、DB2、MySQL、SQL SERVER 等采用行式存储法(Row-based)，在基于行式存储的数据库中， 数据是按照行数据为基础逻辑存储单元进行存储的， 一行中的数据在存储介质中以连续存储形式存在。
   
   行式
   
    1、适合随机的增删改查操作;
    2、需要在行中选取所有属性的查询操作;
    3、需要频繁插入或更新的操作，其操作与索引和行的大小更为相关。
    
   列式：
    
    1、查询过程中，可针对各列的运算并发执行(SMP)，最后在内存中聚合完整记录集，最大可能降低查询响应时间;
    2、可在数据列中高效查找数据，无需维护索引(任何列都能作为索引)，查询过程中能够尽量减少无关IO，避免全表扫描;
    3、因为各列独立存储，且数据类型已知，可以针对该列的数据类型、数据量大小等因素动态选择压缩算法，以提高物理存储利用率;如果某一行的某一列没有数据，那在列存储时，就可以不存储该列的值，这将比行式存储更节省空间。
    

### 压缩格式
    1.节省空间
    2，传输速度快
    config("spark.sql.parquet.compression.codec", "gzip")
    默认的话是snapy
    
 
### SparkSQL 性能优化

    并行度：spark.sql.shuffle.partitions 
    分区字段类型推测： spark.sql.sources.partitionCoIumnTypeInference.enabIed
    
   